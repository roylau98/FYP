{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-17T07:02:11.997979800Z",
     "start_time": "2024-02-17T07:02:11.915004700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, number_of_categories):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # convolution layer\n",
    "        # in_channels must be the same as the number of subcarriers, out_channels can be any value\n",
    "        self.conv1d = nn.Conv1d(in_channels=64, out_channels=40, kernel_size=3, stride=1)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # calculate the in_features\n",
    "        self.fc1 = nn.Linear(in_features=40 * 19, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=number_of_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input tensor size [batch_size, 64 (features), 40 (sequence length)]\n",
    "        x = self.conv1d(x)\n",
    "        # [batch_size, 64 (features), 40 (sequence length)]\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2])  # Adjust the input size based on the output size after convolutions and pooling\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T06:31:41.955259600Z",
     "start_time": "2024-02-17T06:31:41.933256800Z"
    }
   },
   "id": "328841f011159dc2"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class CSIDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.int)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T06:31:43.345004500Z",
     "start_time": "2024-02-17T06:31:43.306314700Z"
    }
   },
   "id": "b7ee4e93d8c47e59"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def importData():\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = [], [], [], [], [], []\n",
    "    for i in range(10):\n",
    "        temp_train = np.load(f'../data/processed_data/{i}_table_train.npy')\n",
    "        temp_val = np.load(f'../data/processed_data/{i}_table_val.npy')\n",
    "        temp_test = np.load(f'../data/processed_data/{i}_table_test.npy')\n",
    "        if i == 0:\n",
    "            X_train = temp_train\n",
    "            X_val = temp_val\n",
    "            X_test = temp_test\n",
    "            y_train = np.array([[i] for x in range(X_train.shape[0])])\n",
    "            y_val = np.array([[i] for x in range(X_val.shape[0])])\n",
    "            y_test = np.array([[i] for x in range(X_test.shape[0])])\n",
    "        else:\n",
    "            X_train = np.append(X_train, temp_train, axis=0)\n",
    "            X_val = np.append(X_val, temp_val, axis=0)\n",
    "            X_test = np.append(X_test, temp_test, axis=0)\n",
    "            y_train = np.append(y_train, np.array([[i] for _ in range(temp_train.shape[0])]), axis=0)\n",
    "            y_val = np.append(y_val, np.array([[i] for _ in range(temp_val.shape[0])]), axis=0)\n",
    "            y_test = np.append(y_test, np.array([[i] for _ in range(temp_test.shape[0])]), axis=0)\n",
    "            \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T06:50:51.722784Z",
     "start_time": "2024-02-17T06:50:51.447918100Z"
    }
   },
   "id": "256be7a9c283785f"
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "class RunningAverage:\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.total / float(self.steps)\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    outputs = np.argmax(outputs.cpu().detach().numpy(), axis=1)\n",
    "    labels = labels.squeeze()\n",
    "    # compare outputs with labels\n",
    "    return np.sum([1 if first == second else 0 for first, second in zip(labels, outputs)]) / float(len(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T07:26:42.356335300Z",
     "start_time": "2024-02-17T07:26:42.341335800Z"
    }
   },
   "id": "6cce562da6c9e367"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "def train(model, optimizer, trainLoader, loss_fn, epoch, iterations):\n",
    "    model.train()\n",
    "    train_loss = RunningAverage()\n",
    "    train_acc = RunningAverage()\n",
    "    \n",
    "    with trange(iterations) as pbar:\n",
    "        for X_batch, y_batch in trainLoader:\n",
    "            y_batch = y_batch.type(torch.LongTensor).squeeze(1)\n",
    "            logits = model(X_batch)\n",
    "            y_pred = F.log_softmax(logits, dim=1) # use this for accuracy \n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.update(loss)\n",
    "            train_acc.update(accuracy(y_pred, y_batch))\n",
    "            pbar.update(1)\n",
    "        \n",
    "    print(f\"Train: Epoch {epoch}: Loss {train_loss()}, Accuracy {train_acc()}\")\n",
    "    \n",
    "def eval(model, valLoader, loss_fn, epoch):\n",
    "    model.eval()\n",
    "    eval_loss = RunningAverage()\n",
    "    eval_acc = RunningAverage()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in valLoader:\n",
    "            y_batch = y_batch.type(torch.LongTensor).squeeze(1)\n",
    "            logits = model(X_batch)\n",
    "            y_pred = F.log_softmax(logits, dim=1) # use this for accuracy \n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            \n",
    "            eval_loss.update(loss)\n",
    "            eval_acc.update(accuracy(y_pred, y_batch))\n",
    "            \n",
    "    print(f\"Eval: Epoch {epoch}: Loss {eval_loss()}, Accuracy {eval_acc()}\")\n",
    "    \n",
    "def test(model, testLoader):\n",
    "    model.eval()\n",
    "    for X_batch, y_batch in testLoader:\n",
    "        print(X_batch.shape)\n",
    "        y_batch = y_batch.type(torch.LongTensor).squeeze(1)\n",
    "        logits = model(X_batch)\n",
    "        y_pred = F.log_softmax(logits, dim=1) # use this for accuracy \n",
    "        \n",
    "    print(f\"Final Accuracy: {accuracy(y_pred, y_batch)}\")\n",
    "\n",
    "    # True positive percentage\n",
    "    percentages = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
    "    outputs = np.argmax(y_pred.cpu().detach().numpy(), axis=1)\n",
    "    labels = y_batch.squeeze()\n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == labels[i]:\n",
    "            percentages[outputs[i]] += 1\n",
    "            \n",
    "    print(percentages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T07:26:23.143775900Z",
     "start_time": "2024-02-17T07:26:23.034777300Z"
    }
   },
   "id": "ed38b6b8a4171fef"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "def train_and_eval(model, epochs, optimizer, trainLoader, valLoader, loss_fn, iterations):\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        train(model, optimizer, trainLoader, loss_fn, epoch, iterations)\n",
    "        eval(model, valLoader, loss_fn, epoch)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T07:15:04.866274300Z",
     "start_time": "2024-02-17T07:15:04.840257700Z"
    }
   },
   "id": "b170265a0723c3c6"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 64, 40])\n",
      "[5 4 2 6 2 9 6 1 0 8 9 6 6 2 7 1 0 7 5 5 9 5 1 0 9 2 7 5 1 0 3 6 1 5 6 4 0\n",
      " 4 8 4 3 0 1 2 4 4 5 8 7 9 5 1 5 3 6 2 1 7 2 0 8 4 2 8 3 4 7 7 4 7 2 1 5 5\n",
      " 3 9 5 5 3 3 7 6 1 3 3 6 9 0 4 8 8 3 3 9 2 4 5 3 9 2 2 3 3 4 0 5 5 0 0 8 1\n",
      " 3 6 9 9 7 7 8 0 7 9 7 3 2 6 7 1 3 2 3 1 4 8 0 8 2 3 9 1 7 6 6 0 3 1 5 1 3\n",
      " 2 3 5 0 3 1 0 9 6 9 1 5 9 5 2 4 1 4 8 3 1 8 6 0 2 9 6 1 8 9 1 2 8 4 6 4 4\n",
      " 3 1 6 5 0 7 6 8 3 4 2 3 2 3 9 2 1 5 6 8 7 2 1 3 7 1 8 8 5 0 9 7 7 7 3 1 6\n",
      " 0 9 4 9 8 0 7 0 6 3 5 4 3 3 4 4 7 5 0 3 1 5 9 9 4 8 5 0 2 6 3 5 9 3 2 9 8\n",
      " 4 7 8 9 1 2 1 8 1 7 2 7 9 6 4 3 0 6 4 6 4 3 5 9 8 6 0 6 4 1 0 6 7 6 3 9 5\n",
      " 0 7 0 5] tensor([5, 4, 3, 6, 2, 9, 6, 4, 0, 8, 9, 6, 6, 2, 7, 1, 0, 7, 3, 5, 9, 5, 1, 0,\n",
      "        9, 2, 7, 5, 1, 0, 3, 0, 1, 5, 6, 4, 0, 4, 8, 4, 2, 0, 1, 5, 4, 4, 8, 8,\n",
      "        7, 9, 5, 1, 5, 3, 0, 2, 1, 7, 2, 0, 8, 4, 2, 8, 3, 4, 7, 7, 4, 7, 3, 1,\n",
      "        5, 5, 3, 9, 5, 5, 3, 7, 7, 6, 1, 3, 3, 6, 9, 0, 4, 8, 8, 3, 3, 9, 2, 4,\n",
      "        5, 8, 9, 6, 2, 8, 7, 4, 0, 2, 5, 9, 0, 8, 1, 3, 6, 6, 9, 7, 7, 8, 0, 7,\n",
      "        9, 7, 2, 2, 6, 7, 1, 2, 2, 8, 1, 4, 8, 0, 8, 2, 3, 9, 1, 7, 6, 6, 0, 8,\n",
      "        1, 5, 1, 3, 2, 3, 5, 0, 3, 1, 0, 9, 6, 9, 1, 5, 9, 5, 2, 4, 1, 4, 8, 8,\n",
      "        1, 8, 6, 0, 2, 9, 6, 1, 8, 9, 2, 2, 8, 4, 6, 4, 4, 5, 1, 6, 5, 0, 7, 6,\n",
      "        8, 3, 4, 2, 3, 2, 3, 9, 2, 1, 5, 6, 3, 7, 2, 1, 3, 7, 1, 5, 8, 5, 0, 9,\n",
      "        3, 3, 7, 3, 1, 6, 0, 9, 4, 9, 8, 0, 7, 0, 6, 8, 5, 4, 3, 2, 4, 4, 7, 5,\n",
      "        0, 7, 1, 5, 9, 9, 4, 3, 8, 0, 2, 6, 8, 5, 9, 3, 2, 9, 8, 4, 7, 2, 9, 1,\n",
      "        2, 1, 8, 1, 7, 2, 7, 0, 6, 4, 3, 0, 6, 4, 6, 4, 3, 5, 9, 8, 0, 0, 6, 4,\n",
      "        1, 0, 6, 7, 6, 7, 9, 5, 6, 7, 9, 5])\n",
      "Final Accuracy: 0.8666666666666667\n",
      "{0: 26, 1: 30, 2: 23, 3: 23, 4: 29, 5: 27, 6: 27, 7: 26, 8: 21, 9: 28}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # \n",
    "    batch_size = 64\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = importData()\n",
    "    trainDataset = CSIDataset(X_train, y_train)\n",
    "    valDataset = CSIDataset(X_val, y_val)\n",
    "    \n",
    "    trainLoader = DataLoader(trainDataset, shuffle=True, batch_size=batch_size)\n",
    "    valLoader = DataLoader(valDataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    number_of_categories = 10\n",
    "    model = CNN1D(number_of_categories)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    if (os.path.isfile(\"model.pth\")):\n",
    "        model.load_state_dict(torch.load('model.pth'))\n",
    "    else:\n",
    "        train_and_eval(model, 100, optimizer, trainLoader, valLoader, loss, (X_train.shape[0]//batch_size) + 1)\n",
    "        \n",
    "    # Simple check with test dataset\n",
    "    model.eval()\n",
    "    testDataset = CSIDataset(X_test, y_test)\n",
    "    testLoader = DataLoader(testDataset, shuffle=True, batch_size=X_test.shape[0])\n",
    "    test(model, testLoader)\n",
    "    \n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    \n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T07:26:25.453103800Z",
     "start_time": "2024-02-17T07:26:25.146128200Z"
    }
   },
   "id": "8f57efd5d9456eb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f3d37068c00c5ddb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
